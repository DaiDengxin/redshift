"""
MALT-style dependency parser
"""
cimport cython
import random
import os.path
from os.path import join as pjoin
import shutil
import json

from libc.stdlib cimport malloc, free, calloc
from libc.string cimport memcpy, memset

from _state cimport *
<<<<<<< HEAD
from io_parse cimport Sentence, Sentences
from io_parse import read_conll, read_pos
from transitions cimport TransitionSystem, transition_to_str 
=======
from sentence cimport Input, Sentence, Token
from transitions cimport Transition, transition, fill_valid, fill_costs
from transitions cimport get_nr_moves, fill_moves
from transitions cimport *
>>>>>>> feature/experiments
from beam cimport Beam
from tagger cimport Tagger
from tagger import write_tagger_config

from features.extractor cimport Extractor
import _parse_features
from _parse_features cimport *

import index.hashes
cimport index.hashes

from learn.perceptron cimport Perceptron

from libc.stdint cimport uint64_t, int64_t


VOCAB_SIZE = 1e6
TAG_SET_SIZE = 50


DEBUG = False 
def set_debug(val):
    global DEBUG
    DEBUG = val


<<<<<<< HEAD
def load_parser(model_dir):
    params = dict([line.split() for line in open(pjoin(model_dir, 'parser.cfg'))])
    train_alg = params['train_alg']
    feat_thresh = int(params['feat_thresh'])
    allow_reattach = params['allow_reattach'] == 'True'
    allow_reduce = params['allow_reduce'] == 'True'
    use_edit = params['use_edit'] == 'True'
    l_labels = params['left_labels']
    r_labels = params['right_labels']
    beam_width = int(params['beam_width'])
    feat_set = params['feat_set']
    ngrams = []
    for ngram_str in params.get('ngrams', '-1').split(','):
        if ngram_str == '-1': continue
        ngrams.append(tuple([int(i) for i in ngram_str.split('_')]))
    auto_pos = params['auto_pos'] == 'True'
    params = {'clean': False, 'train_alg': train_alg,
              'feat_set': feat_set, 'feat_thresh': feat_thresh,
              'vocab_thresh': 1, 'allow_reattach': allow_reattach,
              'use_edit': use_edit, 'allow_reduce': allow_reduce,
              'beam_width': beam_width,
              'ngrams': ngrams,
              'auto_pos': auto_pos}
    if beam_width >= 2:
        parser = BeamParser(model_dir, **params)
    else:
        parser = GreedyParser(model_dir, **params)
    pos_tags = set([int(line.split()[0]) for line in open(pjoin(model_dir, 'pos'))])
    parser.load()
    _, nr_label = parser.moves.set_labels(pos_tags, _parse_labels_str(l_labels),
                            _parse_labels_str(r_labels))
    
=======
def train(train_str, model_dir, n_iter=15, beam_width=8, train_tagger=True,
          feat_set='basic', feat_thresh=10,
          use_edit=False, use_break=False, use_filler=False):
    if os.path.exists(model_dir):
        shutil.rmtree(model_dir)
    os.mkdir(model_dir)
    cdef list sents = [Input.from_conll(s) for s in
                       train_str.strip().split('\n\n') if s.strip()]
    left_labels, right_labels = get_labels(sents)
    Config.write(model_dir, beam_width=beam_width, features=feat_set,
                 feat_thresh=feat_thresh,
                 left_labels=left_labels, right_labels=right_labels,
                 use_edit=use_edit, use_break=use_break, use_filler=use_filler)
    write_tagger_config(model_dir, beam_width=4, features='basic', feat_thresh=feat_thresh)
    parser = Parser(model_dir)
    indices = list(range(len(sents)))
    cdef Input py_sent
    for n in range(n_iter):
        for i in indices:
            py_sent = sents[i]
            parser.tagger.train_sent(py_sent)
            parser.train_sent(py_sent)
        parser.guide.end_train_iter(n, feat_thresh)
        parser.tagger.guide.end_train_iter(n, feat_thresh)
        random.shuffle(indices)
    parser.guide.end_training(pjoin(model_dir, 'model.gz'))
    parser.tagger.guide.end_training(pjoin(model_dir, 'tagger.gz'))
    index.hashes.save_pos_idx(pjoin(model_dir, 'pos'))
    index.hashes.save_label_idx(pjoin(model_dir, 'labels'))
>>>>>>> feature/experiments
    return parser


def get_labels(sents):
    left_labels = set()
    right_labels = set()
    cdef Input sent
    for i, sent in enumerate(sents):
        for j in range(sent.length):
            if sent.c_sent.tokens[j].head > j:
                left_labels.add(sent.c_sent.tokens[j].label)
            else:
                right_labels.add(sent.c_sent.tokens[j].label)
    return list(sorted(left_labels)), list(sorted(right_labels))


class Config(object):
    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)

    @classmethod
    def write(cls, model_dir, **kwargs):
        open(pjoin(model_dir, 'config.json'), 'w').write(json.dumps(kwargs))

    @classmethod
    def read(cls, model_dir):
        return cls(**json.load(open(pjoin(model_dir, 'config.json'))))


def get_templates(feats_str):
    match_feats = []
    templates = _parse_features.arc_hybrid
    if 'disfl' in feats_str:
        templates += _parse_features.disfl
        templates += _parse_features.new_disfl
        templates += _parse_features.suffix_disfl
        templates += _parse_features.extra_labels
        templates += _parse_features.clusters
        templates += _parse_features.edges
        match_feats = _parse_features.match_templates()
    elif 'clusters' in feats_str:
        templates += _parse_features.clusters
    if 'bitags' in feats_str:
        templates += _parse_features.pos_bigrams()
    return templates, match_feats


cdef class Parser:
    cdef object cfg
    cdef Extractor extractor
    cdef Perceptron guide
    cdef Tagger tagger
    cdef size_t beam_width
    cdef int feat_thresh
    cdef Transition* moves
    cdef uint64_t* _features
    cdef size_t* _context
    cdef size_t nr_moves

<<<<<<< HEAD
    def __cinit__(self, model_dir, clean=False, train_alg='static',
                  feat_set="zhang",
                  feat_thresh=0, vocab_thresh=5,
                  allow_reattach=False, allow_reduce=False, use_edit=False,
                  beam_width=1, ngrams=None, auto_pos=False):
        self.model_dir = self.setup_model_dir(model_dir, clean)
        self.feat_set = feat_set
        self.ngrams = ngrams if ngrams is not None else []
        templates = _parse_features.baseline_templates()
        #templates += _parse_features.ngram_feats(self.ngrams)
        if 'disfl' in self.feat_set:
            templates += _parse_features.disfl
            templates += _parse_features.new_disfl
            templates += _parse_features.suffix_disfl
        if 'xlabels' in self.feat_set:
            templates += _parse_features.extra_labels
        if 'stack' in self.feat_set:
            templates += _parse_features.stack_second
        if 'hist' in self.feat_set:
            templates += _parse_features.history
        if 'clusters' in self.feat_set:
            templates += _parse_features.clusters
        if 'bitags' in self.feat_set:
            templates += _parse_features.pos_bigrams()
        if 'edges' in self.feat_set:
            templates += _parse_features.edges
        if 'match' in self.feat_set:
            match_feats = _parse_features.match_templates()
            print "Using %d match feats" % len(match_feats)
        else:
            match_feats = []
        self.extractor = Extractor(templates, match_feats)
        self._features = <uint64_t*>calloc(self.extractor.nr_feat, sizeof(uint64_t))
        self._context = <size_t*>calloc(_parse_features.context_size(), sizeof(size_t))
        self.feat_thresh = feat_thresh
        self.train_alg = train_alg
        self.beam_width = beam_width
        self.moves = TransitionSystem(allow_reattach=allow_reattach,
                                      allow_reduce=allow_reduce, use_edit=use_edit)
        self.auto_pos = auto_pos
        self.say_config()
        self.guide = Perceptron(self.moves.max_class, pjoin(model_dir, 'model.gz'))
        self.tagger = BeamTagger(model_dir, clean=False)

    def setup_model_dir(self, loc, clean):
        if clean and os.path.exists(loc):
            shutil.rmtree(loc)
        if os.path.exists(loc):
            assert os.path.isdir(loc)
        else:
            os.mkdir(loc)
        return loc

    def train(self, str train_str, unlabelled=False, n_iter=15):
        cdef size_t i, j, n
        cdef Sentence* sent
        cdef Sentences held_out_gold
        cdef Sentences held_out_parse

        cdef Sentences sents = read_conll(train_str,
                                          unlabelled=unlabelled)
        self.say_config()
        self.tagger.setup_classes(sents)
        move_classes, nr_label = self.moves.set_labels(*sents.get_labels())
        #self.features.set_nr_label(nr_label)
        self.guide.set_classes(range(move_classes))
        self.write_cfg(pjoin(self.model_dir, 'parser.cfg'))
        if self.beam_width >= 2:
            self.guide.use_cache = True
        indices = list(range(sents.length))
        if not DEBUG:
            # Extra trick: sort by sentence length for first iteration
            indices.sort(key=lambda i: sents.s[i].length)
        for n in range(n_iter):
            for i in indices:
                if DEBUG:
                    print ' '.join(sents.strings[i][0])
                if self.auto_pos:
                    self.tagger.train_sent(sents.s[i])
                if self.train_alg == 'static':
                    self.static_train(n, sents.s[i])
                else:
                    self.dyn_train(n, sents.s[i])
            if self.auto_pos:
                print_train_msg(n, self.tagger.guide.n_corr, self.tagger.guide.total,
                                0, 0)
            print_train_msg(n, self.guide.n_corr, self.guide.total, self.guide.cache.n_hit,
                            self.guide.cache.n_miss)
            self.guide.n_corr = 0
            self.guide.total = 0
            if n % 2 == 1 and self.feat_thresh > 1:
                self.guide.prune(self.feat_thresh)
                self.tagger.guide.prune(self.feat_thresh / 2)
            if n < 3:
                self.guide.reindex()
                self.tagger.guide.reindex()
            random.shuffle(indices)
        if self.auto_pos:
            self.tagger.guide.finalize()
        self.guide.finalize()

    cdef int dyn_train(self, int iter_num, Sentence* sent) except -1:
        raise NotImplementedError

    cdef int static_train(self, int iter_num, Sentence* sent) except -1:
        raise NotImplementedError
    
    def parse_file(self, in_loc, out_loc):
        cdef Sentences sents = read_pos(open(in_loc).read())
        self.guide.nr_class = self.moves.nr_class
        cdef size_t i
        for i in range(sents.length):
            self.parse(sents.s[i])
        sents.write_parses(open(out_loc, 'w'))

    cdef int parse(self, Sentence* sent) except -1:
        raise NotImplementedError

    def save(self):
        self.guide.save(pjoin(self.model_dir, 'model.gz'))
        self.tagger.save()
        index.hashes.save_idx('word', pjoin(self.model_dir, 'words'))
        index.hashes.save_idx('pos', pjoin(self.model_dir, 'pos'))
        index.hashes.save_idx('labels', pjoin(self.model_dir, 'labels'))
   
    def load(self):
        self.guide.load(pjoin(self.model_dir, 'model.gz'), thresh=self.feat_thresh)
        self.tagger.guide.load(pjoin(self.model_dir, 'tagger.gz'), thresh=self.feat_thresh)
        index.hashes.load_idx('word', pjoin(self.model_dir, 'words'))
        index.hashes.load_idx('pos', pjoin(self.model_dir, 'pos'))
        index.hashes.load_idx('label', pjoin(self.model_dir, 'labels'))
   
    def write_cfg(self, loc):
        with open(loc, 'w') as cfg:
            cfg.write(u'model_dir\t%s\n' % self.model_dir)
            cfg.write(u'train_alg\t%s\n' % self.train_alg)
            cfg.write(u'feat_thresh\t%d\n' % self.feat_thresh)
            cfg.write(u'allow_reattach\t%s\n' % self.moves.allow_reattach)
            cfg.write(u'allow_reduce\t%s\n' % self.moves.allow_reduce)
            cfg.write(u'use_edit\t%s\n' % self.moves.use_edit)
            cfg.write(u'left_labels\t%s\n' % ','.join(self.moves.left_labels))
            cfg.write(u'right_labels\t%s\n' % ','.join(self.moves.right_labels))
            cfg.write(u'beam_width\t%d\n' % self.beam_width)
            cfg.write(u'auto_pos\t%s\n' % self.auto_pos)
            #if not self.features.ngrams:
            #    cfg.write(u'ngrams\t-1\n')
            #else:
            #    ngram_strs = ['_'.join([str(i) for i in ngram])
            #                  for ngram in self.features.ngrams]
            #    cfg.write(u'ngrams\t%s\n' % u','.join(ngram_strs))
            cfg.write(u'feat_set\t%s\n' % self.feat_set)

    def __dealloc__(self):
        pass


cdef class BeamParser(BaseParser):
    cdef int parse(self, Sentence* sent) except -1:
        cdef State* s
        cdef Beam beam = Beam(self.moves, self.beam_width, sent.length)
        cdef size_t p_idx
        cdef Kernel* kernel
        cdef double** beam_scores = <double**>malloc(beam.k * sizeof(double*))
        if self.auto_pos:
            self.tagger.tag(sent)
=======
    def __cinit__(self, model_dir):
        assert os.path.exists(model_dir) and os.path.isdir(model_dir)
        self.cfg = Config.read(model_dir)
        self.extractor = Extractor(*get_templates(self.cfg.features))
        self._features = <uint64_t*>calloc(self.extractor.nr_feat, sizeof(uint64_t))
        self._context = <size_t*>calloc(_parse_features.context_size(), sizeof(size_t))

        self.feat_thresh = self.cfg.feat_thresh
        self.beam_width = self.cfg.beam_width

        if os.path.exists(pjoin(model_dir, 'labels')):
            index.hashes.load_label_idx(pjoin(model_dir, 'labels'))
        self.nr_moves = get_nr_moves(self.cfg.left_labels, self.cfg.right_labels,
                                     self.cfg.use_edit, self.cfg.use_break,
                                     self.cfg.use_filler)
        self.moves = <Transition*>calloc(self.nr_moves, sizeof(Transition))
        fill_moves(self.cfg.left_labels, self.cfg.right_labels, self.cfg.use_edit,
                   self.cfg.use_break, self.cfg.use_filler, self.moves)
        
        self.guide = Perceptron(self.nr_moves, pjoin(model_dir, 'model.gz'))
        if os.path.exists(pjoin(model_dir, 'model.gz')):
            self.guide.load(pjoin(model_dir, 'model.gz'), thresh=int(self.cfg.feat_thresh))
        if os.path.exists(pjoin(model_dir, 'pos')):
            index.hashes.load_pos_idx(pjoin(model_dir, 'pos'))
        self.tagger = Tagger(model_dir)

    cpdef int parse(self, Input py_sent) except -1:
        cdef Sentence* sent = py_sent.c_sent
        cdef size_t p_idx, i
        if self.tagger:
            self.tagger.tag(py_sent)
        cdef Beam beam = Beam(self.beam_width, <size_t>self.moves, self.nr_moves,
                              py_sent)

>>>>>>> feature/experiments
        self.guide.cache.flush()
        while not beam.is_finished:
            for i in range(beam.bsize):
                if not beam.beam[i].is_finished:
                    self._predict(beam.beam[i], beam.moves[i], sent.lattice)
                # The False flag tells it to allow non-gold predictions
                beam.enqueue(i, False)
            beam.extend()
        beam.fill_parse(sent.tokens)
        sent.score = beam.beam[0].score

    cdef int _predict(self, State* s, Transition* classes, Step* lattice) except -1:
        cdef bint cache_hit = False
        if s.is_finished:
            return 0
        fill_slots(s)
        scores = self.guide.cache.lookup(sizeof(SlotTokens), &s.slots, &cache_hit)
        if not cache_hit:
            fill_context(self._context, &s.slots, s.parse, lattice)
            self.extractor.extract(self._features, self._context)
            self.guide.fill_scores(self._features, scores)
        fill_valid(s, classes, self.nr_moves)
        for i in range(self.nr_moves):
            classes[i].score = scores[i]

    cdef int train_sent(self, Input py_sent) except -1:
        cdef size_t i
        cdef size_t nr_move = sent.n * 3
        cdef Transition[1000] g_hist
        cdef Transition[1000] p_hist
        cdef Sentence* sent = py_sent.c_sent
        
        cdef size_t* gold_tags = <size_t*>calloc(sent.n, sizeof(size_t))
        for i in range(sent.n):
            gold_tags[i] = sent.tokens[i].tag
        if self.tagger:
            self.tagger.tag(py_sent)
        g_beam = Beam(self.beam_width, <size_t>self.moves, self.nr_moves, py_sent)
        p_beam = Beam(self.beam_width, <size_t>self.moves, self.nr_moves, py_sent)
        cdef Token* gold_parse = sent.tokens
        cdef double delta = 0
        cdef double max_violn = -1
        cdef size_t pt = 0
        cdef size_t gt = 0
        cdef State* p
        cdef State* g
        cdef Transition* moves
        self.guide.cache.flush()
        words = py_sent.words
        #print words
        move_strs = ['?', 'S', 'L', 'R', 'E', 'F', 'B']
        while not p_beam.is_finished and not g_beam.is_finished:
            for i in range(p_beam.bsize):
                self._predict(p_beam.beam[i], p_beam.moves[i], sent.lattice)
                # Fill costs so we can see whether the prediction is gold-standard
                #print 'ignore',
                fill_costs(p_beam.beam[i], p_beam.moves[i], self.nr_moves, gold_parse)
                # The False flag tells it to allow non-gold predictions
                p_beam.enqueue(i, False)
            p_beam.extend()
            for i in range(g_beam.bsize):
                g = g_beam.beam[i]
                moves = g_beam.moves[i]
                self._predict(g, moves, sent.lattice)
                # Constrain this beam to only gold candidates
                #print g.stack_len
                #print words[g.second], words[g.top], '|', words[g.i]
                #print sent.tokens[g.second].is_edit, sent.tokens[g.top].is_edit, sent.tokens[g.i].is_edit
                fill_costs(g, moves, self.nr_moves, gold_parse)
                g_beam.enqueue(i, True)
            g_beam.extend()
            g = g_beam.beam[0]; p = p_beam.beam[0] 
            #print 'Move taken: ', move_strs[g.history[g.m-1].move]
            delta = p.score - g.score
            if delta >= max_violn and p.cost >= 1:
                max_violn = delta
                pt = p.m
                gt = g.m
                memcpy(p_hist, p.history, pt * sizeof(Transition))
                memcpy(g_hist, g.history, gt * sizeof(Transition))
        if max_violn >= 0:
            counted = self._count_feats(sent, pt, gt, p_hist, g_hist)
            self.guide.batch_update(counted)
            # TODO: We should tick the epoch here if max_violn == 0, right?
        #else:
        #    self.guide.now += 1
        for i in range(sent.n):
            sent.tokens[i].tag = gold_tags[i]
        free(gold_tags)

    cdef dict _count_feats(self, Sentence* sent, size_t pt, size_t gt,
                           Transition* phist, Transition* ghist):
        cdef size_t d, i, f
        cdef uint64_t* feats
        cdef size_t clas
        cdef State* gold_state = init_state(sent)
        cdef State* pred_state = init_state(sent)
        # Find where the states diverge
        cdef dict counts = {}
        for clas in range(self.nr_moves):
            counts[clas] = {}
        cdef bint seen_diff = False
        g_inc = 1.0
        p_inc = -1.0
        for i in range(max((pt, gt))):
            self.guide.total += 1
            if not seen_diff and ghist[i].clas == phist[i].clas:
                self.guide.n_corr += 1
                transition(&ghist[i], gold_state)
                transition(&phist[i], pred_state)
                continue
            seen_diff = True
            if i < gt:
                self._inc_feats(counts[ghist[i].clas], gold_state, sent.lattice, g_inc)
                transition(&ghist[i], gold_state)
            if i < pt:
                self._inc_feats(counts[phist[i].clas], pred_state, sent.lattice, p_inc)
                transition(&phist[i], pred_state)
        free_state(gold_state)
        free_state(pred_state)
        return counts

    cdef int _inc_feats(self, dict counts, State* s, Step* lattice, double inc) except -1:
        fill_slots(s)
        fill_context(self._context, &s.slots, s.parse, lattice)
        self.extractor.extract(self._features, self._context)
 
        cdef size_t f = 0
        while self._features[f] != 0:
            if self._features[f] not in counts:
                counts[self._features[f]] = 0
            counts[self._features[f]] += inc
            f += 1
